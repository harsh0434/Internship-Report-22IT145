# -*- coding: utf-8 -*-
"""Internship Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yFQNhenmc4r61AnXs7uljzR2X2ZWgbip
"""

# ==========================
# EARTHQUAKE DATA ANALYSIS
# ==========================

# ðŸ“Œ Step 1: Install dependencies
!pip install openpyxl

# ðŸ“Œ Step 2: Upload your Excel dataset
from google.colab import files
uploaded = files.upload()   # Choose your Excel file (e.g., earthquake.xlsx)

# Replace with the uploaded filename
INPUT_PATH = list(uploaded.keys())[0]

# ðŸ“Œ Step 3: Import required libraries
import os
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from IPython.display import display

# Create output folder
BASE_DIR = "earthquake_analysis"
os.makedirs(BASE_DIR, exist_ok=True)

# ðŸ“Œ Step 4: Load dataset
df = pd.read_excel(INPUT_PATH, sheet_name=0)

# Standardize column names
original_cols = df.columns.tolist()
df.columns = [str(c).strip() for c in df.columns]
df.columns = [c.lower().replace(" ", "_") for c in df.columns]

print("âœ… Data loaded successfully!")
print("Shape:", df.shape)
display(df.head(10))

# ðŸ“Œ Step 5: Map standard earthquake columns
col_map = {
    "latitude": ["latitude", "lat", "y"],
    "longitude": ["longitude", "lon", "long", "x"],
    "depth_km": ["depth_km", "depth", "dep"],
    "magnitude": ["magnitude", "mag", "ml", "mw", "mb"],
    "place": ["place", "location", "region", "area", "site", "name"],
    "time": ["time", "datetime", "date", "origin_time", "event_time", "timestamp"],
}

resolved = {}
for std, candidates in col_map.items():
    for c in df.columns:
        if c in candidates or any(c.startswith(x) for x in candidates):
            resolved[std] = c
            break

# Parse time
time_col = resolved.get("time")
if time_col:
    df[time_col] = pd.to_datetime(df[time_col], errors="coerce", utc=True)
    df["year"]   = df[time_col].dt.year
    df["month"]  = df[time_col].dt.month
    df["day"]    = df[time_col].dt.day
    df["year_month"] = df[time_col].dt.to_period("M").astype(str)

# Convert to numeric
for key in ["latitude", "longitude", "depth_km", "magnitude"]:
    if key in resolved:
        df[resolved[key]] = pd.to_numeric(df[resolved[key]], errors="coerce")

# ðŸ“Œ Step 6: Missing values
print("\nðŸ”Ž Missing Values Summary:")
print(df.isna().sum())

# ðŸ“Œ Step 7: Descriptive stats
print("\nðŸ“Š Descriptive Statistics:")
display(df.describe(include="all").transpose())

if "magnitude" in resolved:
    print("\nMagnitude Statistics:")
    display(df[resolved["magnitude"]].describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99]))

if "depth_km" in resolved:
    print("\nDepth Statistics:")
    display(df[resolved["depth_km"]].describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99]))

# ðŸ“Œ Step 8: Correlation Heatmap
num_df = df.select_dtypes(include=[np.number])
if num_df.shape[1] >= 2:
    corr = num_df.corr(numeric_only=True)
    plt.figure(figsize=(8,6))
    im = plt.imshow(corr.values, interpolation='nearest')
    plt.xticks(np.arange(corr.shape[1]), corr.columns, rotation=45, ha='right')
    plt.yticks(np.arange(corr.shape[0]), corr.index)
    plt.colorbar(im)
    plt.title("Correlation Heatmap")
    plt.show()

# ðŸ“Œ Step 9: Visualizations
def save_and_show_plot(fig, name):
    path = os.path.join(BASE_DIR, name)
    fig.savefig(path, dpi=200, bbox_inches="tight")
    plt.show()

# Magnitude histogram
if "magnitude" in resolved:
    fig = plt.figure(figsize=(7,5))
    plt.hist(df[resolved["magnitude"]].dropna(), bins=30)
    plt.xlabel("Magnitude"); plt.ylabel("Frequency"); plt.title("Magnitude Distribution")
    save_and_show_plot(fig, "magnitude_hist.png")

# Depth histogram
if "depth_km" in resolved:
    fig = plt.figure(figsize=(7,5))
    plt.hist(df[resolved["depth_km"]].dropna(), bins=30)
    plt.xlabel("Depth (km)"); plt.ylabel("Frequency"); plt.title("Depth Distribution")
    save_and_show_plot(fig, "depth_hist.png")

# Daily counts
if time_col:
    ts = df.dropna(subset=[time_col]).copy()
    ts["date_only"] = ts[time_col].dt.floor("D")
    daily_counts = ts.groupby("date_only").size()
    fig = plt.figure(figsize=(10,4))
    plt.plot(daily_counts.index, daily_counts.values)
    plt.title("Daily Earthquake Counts"); plt.xlabel("Date"); plt.ylabel("Count")
    save_and_show_plot(fig, "daily_counts.png")

# Magnitude vs Depth
if "magnitude" in resolved and "depth_km" in resolved:
    fig = plt.figure(figsize=(6,5))
    plt.scatter(df[resolved["magnitude"]], df[resolved["depth_km"]], alpha=0.6)
    plt.xlabel("Magnitude"); plt.ylabel("Depth (km)"); plt.title("Magnitude vs Depth")
    plt.gca().invert_yaxis()
    save_and_show_plot(fig, "mag_vs_depth.png")

# Epicenter scatter
if "latitude" in resolved and "longitude" in resolved:
    fig = plt.figure(figsize=(6,5))
    plt.scatter(df[resolved["longitude"]], df[resolved["latitude"]], s=10, alpha=0.5)
    plt.xlabel("Longitude"); plt.ylabel("Latitude"); plt.title("Epicenters")
    save_and_show_plot(fig, "epicenters_scatter.png")

# ðŸ“Œ Step 10: Save cleaned dataset
clean_path = os.path.join(BASE_DIR, "earthquakes_cleaned.csv")
df.to_csv(clean_path, index=False)

# --- Key Insights ---
if "magnitude" in resolved:
    max_quake = df.loc[df[resolved["magnitude"]].idxmax()]
    min_quake = df.loc[df[resolved["magnitude"]].idxmin()]
    if time_col:
        print(f"ðŸ”¹ Strongest earthquake: {max_quake[resolved['magnitude']]} at depth {max_quake.get(resolved.get('depth_km'),'NA')} km on {max_quake.get(time_col)}")
        print(f"ðŸ”¹ Weakest earthquake: {min_quake[resolved['magnitude']]} at depth {min_quake.get(resolved.get('depth_km'),'NA')} km on {min_quake.get(time_col)}")
    else:
        print(f"ðŸ”¹ Strongest earthquake: {max_quake[resolved['magnitude']]} at depth {max_quake.get(resolved.get('depth_km'),'NA')} km")
        print(f"ðŸ”¹ Weakest earthquake: {min_quake[resolved['magnitude']]} at depth {min_quake.get(resolved.get('depth_km'),'NA')} km")

print("\nâœ… Analysis complete! All plots saved in:", BASE_DIR)